{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmiflMXNAoMH3cs7eH1p9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palekars/Data-Scientist-Course-2025/blob/main/Multiple_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BzAZXJwG6DAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "96thhRjr6FDc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05e71a8b",
        "outputId": "37325e4a-bcda-4782-f392-4b5d66b9fcd7"
      },
      "source": [
        "df = pd.read_csv(\"/content/1.02.+Multiple+linear+regression.csv\")\n",
        "display(df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    SAT   GPA  Rand 1,2,3\n",
              "0  1714  2.40           1\n",
              "1  1664  2.52           3\n",
              "2  1760  2.54           3\n",
              "3  1685  2.74           3\n",
              "4  1693  2.83           2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bb15e2e-1dfb-4ca6-ac17-30ebb0ae6a43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAT</th>\n",
              "      <th>GPA</th>\n",
              "      <th>Rand 1,2,3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1714</td>\n",
              "      <td>2.40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1664</td>\n",
              "      <td>2.52</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1760</td>\n",
              "      <td>2.54</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1685</td>\n",
              "      <td>2.74</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1693</td>\n",
              "      <td>2.83</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bb15e2e-1dfb-4ca6-ac17-30ebb0ae6a43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5bb15e2e-1dfb-4ca6-ac17-30ebb0ae6a43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5bb15e2e-1dfb-4ca6-ac17-30ebb0ae6a43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5002e478-c6f3-4f6f-8f12-2b92f5e113d1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5002e478-c6f3-4f6f-8f12-2b92f5e113d1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5002e478-c6f3-4f6f-8f12-2b92f5e113d1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"SAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36,\n        \"min\": 1664,\n        \"max\": 1760,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1664,\n          1693,\n          1760\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17487138130637622,\n        \"min\": 2.4,\n        \"max\": 2.83,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.52,\n          2.83,\n          2.54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rand 1,2,3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "G5UV1zJW77RC",
        "outputId": "e75c9b33-35f9-456e-c052-d1a95c9fa6f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               SAT        GPA  Rand 1,2,3\n",
              "count    84.000000  84.000000   84.000000\n",
              "mean   1845.273810   3.330238    2.059524\n",
              "std     104.530661   0.271617    0.855192\n",
              "min    1634.000000   2.400000    1.000000\n",
              "25%    1772.000000   3.190000    1.000000\n",
              "50%    1846.000000   3.380000    2.000000\n",
              "75%    1934.000000   3.502500    3.000000\n",
              "max    2050.000000   3.810000    3.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b018ce96-703c-4d2a-bb2b-313b044c6bad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAT</th>\n",
              "      <th>GPA</th>\n",
              "      <th>Rand 1,2,3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>84.000000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>84.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1845.273810</td>\n",
              "      <td>3.330238</td>\n",
              "      <td>2.059524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.530661</td>\n",
              "      <td>0.271617</td>\n",
              "      <td>0.855192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1634.000000</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1772.000000</td>\n",
              "      <td>3.190000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1846.000000</td>\n",
              "      <td>3.380000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1934.000000</td>\n",
              "      <td>3.502500</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2050.000000</td>\n",
              "      <td>3.810000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b018ce96-703c-4d2a-bb2b-313b044c6bad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b018ce96-703c-4d2a-bb2b-313b044c6bad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b018ce96-703c-4d2a-bb2b-313b044c6bad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-78b81ad8-0aab-4a8a-8df9-bef22e0fc15b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78b81ad8-0aab-4a8a-8df9-bef22e0fc15b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-78b81ad8-0aab-4a8a-8df9-bef22e0fc15b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"SAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 820.0582288582201,\n        \"min\": 84.0,\n        \"max\": 2050.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1845.2738095238096,\n          1846.0,\n          84.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.71613150729136,\n        \"min\": 0.27161709490036023,\n        \"max\": 84.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.330238095238095,\n          3.38,\n          84.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rand 1,2,3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.058844770624233,\n        \"min\": 0.8551923686388248,\n        \"max\": 84.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          84.0,\n          2.0595238095238093,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "f230af88",
        "outputId": "df179a39-657b-44ac-a2fc-b85a8a152d8c"
      },
      "source": [
        "# Define the dependent and independent variables\n",
        "y = df['GPA']\n",
        "x = df[['SAT', 'Rand 1,2,3']]\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "x = sm.add_constant(x)\n",
        "\n",
        "# Create and fit the model\n",
        "model = sm.OLS(y, x).fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "display(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                    GPA   R-squared:                       0.407\n",
              "Model:                            OLS   Adj. R-squared:                  0.392\n",
              "Method:                 Least Squares   F-statistic:                     27.76\n",
              "Date:                Sat, 30 Aug 2025   Prob (F-statistic):           6.58e-10\n",
              "Time:                        07:41:12   Log-Likelihood:                 12.720\n",
              "No. Observations:                  84   AIC:                            -19.44\n",
              "Df Residuals:                      81   BIC:                            -12.15\n",
              "Df Model:                           2                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const          0.2960      0.417      0.710      0.480      -0.533       1.125\n",
              "SAT            0.0017      0.000      7.432      0.000       0.001       0.002\n",
              "Rand 1,2,3    -0.0083      0.027     -0.304      0.762      -0.062       0.046\n",
              "==============================================================================\n",
              "Omnibus:                       12.992   Durbin-Watson:                   0.948\n",
              "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               16.364\n",
              "Skew:                          -0.731   Prob(JB):                     0.000280\n",
              "Kurtosis:                       4.594   Cond. No.                     3.33e+04\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 3.33e+04. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>           <td>GPA</td>       <th>  R-squared:         </th> <td>   0.407</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.392</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.76</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Sat, 30 Aug 2025</td> <th>  Prob (F-statistic):</th> <td>6.58e-10</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>07:41:12</td>     <th>  Log-Likelihood:    </th> <td>  12.720</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>    84</td>      <th>  AIC:               </th> <td>  -19.44</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>    81</td>      <th>  BIC:               </th> <td>  -12.15</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th>      <td>    0.2960</td> <td>    0.417</td> <td>    0.710</td> <td> 0.480</td> <td>   -0.533</td> <td>    1.125</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>SAT</th>        <td>    0.0017</td> <td>    0.000</td> <td>    7.432</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Rand 1,2,3</th> <td>   -0.0083</td> <td>    0.027</td> <td>   -0.304</td> <td> 0.762</td> <td>   -0.062</td> <td>    0.046</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>12.992</td> <th>  Durbin-Watson:     </th> <td>   0.948</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>  16.364</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td>-0.731</td> <th>  Prob(JB):          </th> <td>0.000280</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 4.594</td> <th>  Cond. No.          </th> <td>3.33e+04</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.33e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &       GPA        & \\textbf{  R-squared:         } &     0.407   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.392   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     27.76   \\\\\n\\textbf{Date:}             & Sat, 30 Aug 2025 & \\textbf{  Prob (F-statistic):} &  6.58e-10   \\\\\n\\textbf{Time:}             &     07:41:12     & \\textbf{  Log-Likelihood:    } &    12.720   \\\\\n\\textbf{No. Observations:} &          84      & \\textbf{  AIC:               } &    -19.44   \\\\\n\\textbf{Df Residuals:}     &          81      & \\textbf{  BIC:               } &    -12.15   \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{const}      &       0.2960  &        0.417     &     0.710  &         0.480        &       -0.533    &        1.125     \\\\\n\\textbf{SAT}        &       0.0017  &        0.000     &     7.432  &         0.000        &        0.001    &        0.002     \\\\\n\\textbf{Rand 1,2,3} &      -0.0083  &        0.027     &    -0.304  &         0.762        &       -0.062    &        0.046     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 12.992 & \\textbf{  Durbin-Watson:     } &    0.948  \\\\\n\\textbf{Prob(Omnibus):} &  0.002 & \\textbf{  Jarque-Bera (JB):  } &   16.364  \\\\\n\\textbf{Skew:}          & -0.731 & \\textbf{  Prob(JB):          } & 0.000280  \\\\\n\\textbf{Kurtosis:}      &  4.594 & \\textbf{  Cond. No.          } & 3.33e+04  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 3.33e+04. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bcb052"
      },
      "source": [
        "### Explanation of R-squared and Adjusted R-squared\n",
        "\n",
        "In the context of a regression model, R-squared and Adjusted R-squared are metrics used to evaluate how well the independent variables explain the variation in the dependent variable.\n",
        "\n",
        "**R-squared (RÂ²)**\n",
        "\n",
        "*   **Definition**: R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by the independent variables in a regression model.\n",
        "*   **Interpretation**: It ranges from 0 to 1 (or 0% to 100%). A higher R-squared value indicates that more of the variation in the dependent variable can be explained by the independent variables. For example, an R-squared of 0.407 means that approximately 40.7% of the variation in 'GPA' can be explained by 'SAT' and 'Rand 1,2,3' in this model.\n",
        "*   **Limitation**: R-squared can be misleading when comparing models with different numbers of independent variables. Adding more independent variables, even if they are not significant, will generally increase the R-squared value.\n",
        "\n",
        "**Adjusted R-squared**\n",
        "\n",
        "*   **Definition**: Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. It penalizes the addition of unnecessary independent variables.\n",
        "*   **Interpretation**: It is generally lower than R-squared. Adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It is a better measure for comparing models with different numbers of independent variables. In this case, the Adjusted R-squared of 0.392 is slightly lower than the R-squared, which is expected when adding independent variables.\n",
        "*   **Usefulness**: Adjusted R-squared is particularly useful when building multiple regression models where you are considering several potential independent variables. It helps in selecting a model that provides the best fit without being overly complex.\n",
        "\n",
        "In summary, while R-squared tells you how much of the variation in the dependent variable is explained by your model, Adjusted R-squared provides a more accurate picture, especially when comparing models with different numbers of predictors, by accounting for the number of independent variables used."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IeEHklXc9f1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce9e3b5"
      },
      "source": [
        "### Assumptions of Linear Regression\n",
        "\n",
        "Linear regression models make several assumptions about the data to ensure the validity and reliability of the results. It's important to check these assumptions before interpreting the model.\n",
        "\n",
        "Here are the key assumptions:\n",
        "\n",
        "1.  **Linearity**: The relationship between the independent variables and the dependent variable is linear. This means the change in the dependent variable is proportional to the change in the independent variables. You can check this assumption by plotting the dependent variable against each independent variable.\n",
        "2.  **Independence of Errors (No Autocorrelation)**: The errors (residuals) of the model are independent of each other. This means that the error for one observation does not influence the error for another observation. This assumption is particularly important in time series data. The Durbin-Watson statistic in the model summary can help detect autocorrelation.\n",
        "3.  **Homoscedasticity (Constant Variance of Errors)**: The variance of the errors is constant across all levels of the independent variables. In other words, the spread of the residuals is roughly the same throughout the range of the predicted values. You can check this assumption by plotting the residuals against the predicted values. Heteroscedasticity (non-constant variance) can affect the standard errors of the coefficients.\n",
        "4.  **Normality of Errors**: The errors (residuals) of the model are normally distributed. This assumption is important for hypothesis testing and confidence intervals. You can check this assumption by looking at a histogram or a Q-Q plot of the residuals.\n",
        "5.  **No Multicollinearity**: The independent variables are not highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each independent variable on the dependent variable and can lead to unstable coefficient estimates. You can check for multicollinearity by examining the correlation matrix of your independent variables or calculating Variance Inflation Factors (VIF).\n",
        "\n",
        "Checking these assumptions helps ensure that your linear regression model is appropriate for your data and that the results you obtain are valid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205134ee"
      },
      "source": [
        "### Linearity Assumption Explained in Detail\n",
        "\n",
        "The linearity assumption is one of the fundamental assumptions of linear regression. It states that there must be a **linear relationship** between the independent variables and the dependent variable.\n",
        "\n",
        "**What does a Linear Relationship Mean?**\n",
        "\n",
        "A linear relationship means that the change in the dependent variable is directly proportional to the change in the independent variable(s). If you were to plot the dependent variable against an independent variable, the points should roughly form a straight line (or a hyperplane in the case of multiple independent variables).\n",
        "\n",
        "Mathematically, in a simple linear regression with one independent variable ($X$) and a dependent variable ($Y$), the model is represented as:\n",
        "\n",
        "$Y = \\beta_0 + \\beta_1X + \\epsilon$\n",
        "\n",
        "Where:\n",
        "*   $Y$ is the dependent variable.\n",
        "*   $X$ is the independent variable.\n",
        "*   $\\beta_0$ is the y-intercept (the value of Y when X is 0).\n",
        "*   $\\beta_1$ is the slope of the line (the change in Y for a one-unit change in X).\n",
        "*   $\\epsilon$ is the error term (the part of Y that the model cannot explain).\n",
        "\n",
        "In a multiple linear regression with multiple independent variables ($X_1, X_2, ..., X_k$), the model is:\n",
        "\n",
        "$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\epsilon$\n",
        "\n",
        "The linearity assumption implies that the $\\beta$ coefficients are constant and do not depend on the values of the independent variables.\n",
        "\n",
        "**Why is Linearity Important?**\n",
        "\n",
        "If the relationship between the variables is not linear, fitting a linear model will not accurately capture the underlying pattern in the data. This can lead to:\n",
        "\n",
        "*   **Biased Coefficient Estimates**: The estimated $\\beta$ coefficients may not accurately reflect the true relationship between the variables.\n",
        "*   **Inaccurate Predictions**: The model's predictions for the dependent variable will be unreliable, especially for values of the independent variables outside the range of the observed data.\n",
        "*   **Misleading Interpretations**: The interpretations of the model's results (e.g., the effect of an independent variable on the dependent variable) can be incorrect.\n",
        "\n",
        "**How to Check the Linearity Assumption?**\n",
        "\n",
        "You can check the linearity assumption using several methods:\n",
        "\n",
        "1.  **Scatter Plots**: Plot the dependent variable against each independent variable individually. Look for a roughly linear pattern. If the relationship appears curved or non-linear, the assumption is violated.\n",
        "2.  **Residual Plots**: Plot the residuals (the differences between the observed and predicted values of the dependent variable) against the predicted values or the independent variables. If the linearity assumption holds, the residuals should be randomly scattered around zero with no discernible pattern. A curved pattern in the residual plot suggests non-linearity.\n",
        "3.  **Component-Plus-Residual Plots (Partial Regression Plots)**: These plots show the relationship between the dependent variable and an independent variable after accounting for the effects of other independent variables in the model. They can help identify non-linearity or influential points.\n",
        "\n",
        "**What to Do if the Linearity Assumption is Violated?**\n",
        "\n",
        "If you find that the linearity assumption is violated, you have a few options:\n",
        "\n",
        "1.  **Transform the Variables**: You can try transforming one or both variables to create a more linear relationship. Common transformations include taking the logarithm, square root, or reciprocal of the variables.\n",
        "2.  **Add Polynomial Terms**: If the relationship is curved, you can add polynomial terms (e.g., $X^2$, $X^3$) to the model to capture the non-linear pattern.\n",
        "3.  **Use Non-Linear Regression**: If the relationship is inherently non-linear and transformations or polynomial terms don't work, you might need to use a non-linear regression model that is more appropriate for the data.\n",
        "4.  **Consider Other Models**: Linear regression might not be the right model for your data. You might need to consider other types of models that can handle non-linear relationships, such as generalized additive models (GAMs) or tree-based models.\n",
        "\n",
        "In summary, the linearity assumption is crucial for the validity of linear regression results. It's essential to check this assumption and address any violations to ensure that your model accurately represents the relationship between the variables and provides reliable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26291c6c"
      },
      "source": [
        "### Normality of Errors Assumption Explained in Detail\n",
        "\n",
        "The normality assumption in linear regression states that the **errors (residuals)** of the model are **normally distributed**.\n",
        "\n",
        "**What does Normally Distributed Errors Mean?**\n",
        "\n",
        "Normally distributed errors mean that if you were to plot the frequency distribution of the residuals, it would approximate a bell-shaped curve (a normal distribution). In a normal distribution, the errors are symmetrically distributed around a mean of zero, with most errors close to zero and fewer errors further away.\n",
        "\n",
        "Mathematically, this assumption can be expressed as $\\epsilon \\sim N(0, \\sigma^2)$, where $\\epsilon$ represents the error term, $N$ denotes a normal distribution, 0 is the mean of the errors, and $\\sigma^2$ is the variance of the errors.\n",
        "\n",
        "**Why is Normality of Errors Important?**\n",
        "\n",
        "The normality of errors assumption is important for **hypothesis testing** and **confidence intervals** in linear regression.\n",
        "\n",
        "*   **Hypothesis Testing**: The p-values associated with the coefficients in the regression summary (which help determine the statistical significance of the independent variables) are calculated based on the assumption that the errors are normally distributed. If this assumption is violated, the p-values may not be accurate, leading to incorrect conclusions about the significance of the predictors.\n",
        "*   **Confidence Intervals**: The confidence intervals for the coefficients are also derived under the assumption of normally distributed errors. If the assumption is not met, the confidence intervals may be wider or narrower than they should be, affecting the precision of the coefficient estimates.\n",
        "\n",
        "It's worth noting that the normality assumption is **less critical** for estimating the regression coefficients themselves (the $\\beta$ values) in large sample sizes due to the Central Limit Theorem. However, it remains important for the validity of statistical inferences.\n",
        "\n",
        "**How to Check the Normality of Errors Assumption?**\n",
        "\n",
        "You can check the normality of errors assumption using several methods:\n",
        "\n",
        "1.  **Histogram of Residuals**: Plot a histogram of the residuals. Visually inspect if the distribution is roughly symmetric and bell-shaped.\n",
        "2.  **Q-Q Plot (Quantile-Quantile Plot)**: A Q-Q plot compares the quantiles of the residuals to the quantiles of a theoretical normal distribution. If the residuals are normally distributed, the points on the Q-Q plot should fall approximately along a straight line.\n",
        "3.  **Statistical Tests**: Several statistical tests can be used to formally test for normality, such as the Shapiro-Wilk test, Anderson-Darling test, or Kolmogorov-Smirnov test. However, these tests can be sensitive to sample size, and visual inspection of plots is often sufficient, especially for larger datasets.\n",
        "\n",
        "**What to Do if the Normality of Errors Assumption is Violated?**\n",
        "\n",
        "If you find that the normality of errors assumption is violated, you have a few options:\n",
        "\n",
        "1.  **Transform the Dependent Variable**: Transforming the dependent variable (e.g., using a logarithmic or square root transformation) can sometimes help normalize the distribution of the errors.\n",
        "2.  **Identify and Address Outliers**: Outliers in the data can significantly affect the normality of residuals. Identifying and appropriately handling outliers (e.g., removing them if they are data errors, or using robust regression methods) can improve normality.\n",
        "3.  **Use Non-Parametric Methods**: If the errors are severely non-normal and transformations don't work, you might consider using non-parametric regression methods that do not rely on the assumption of normality.\n",
        "4.  **Consider Other Models**: If the underlying data generating process is not linear, a different type of model might be more appropriate, which could also address the non-normality of errors.\n",
        "\n",
        "In summary, while the normality of errors is primarily important for valid statistical inference (p-values and confidence intervals), checking and addressing violations of this assumption is good practice to ensure the reliability of your linear regression results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eeb34d1"
      },
      "source": [
        "### Homoscedasticity (Constant Variance of Errors) Assumption Explained in Detail\n",
        "\n",
        "The homoscedasticity assumption in linear regression states that the **variance of the errors (residuals)** is **constant** across all levels of the independent variables. The opposite of homoscedasticity is **heteroscedasticity**, where the variance of the errors is not constant.\n",
        "\n",
        "**What does Constant Variance of Errors Mean?**\n",
        "\n",
        "Constant variance of errors means that the spread or dispersion of the residuals is roughly the same for all values of the independent variables and for all predicted values of the dependent variable. If you were to plot the residuals against the predicted values or an independent variable, the points should be scattered randomly around zero with a consistent width or band.\n",
        "\n",
        "Mathematically, this assumption can be expressed as $Var(\\epsilon_i | X_i) = \\sigma^2$ for all observations $i$, where $Var(\\epsilon_i | X_i)$ is the variance of the error term for observation $i$ given the independent variables $X_i$, and $\\sigma^2$ is a constant variance.\n",
        "\n",
        "**Why is Homoscedasticity Important?**\n",
        "\n",
        "Homoscedasticity is important because it affects the **efficiency and validity of the coefficient estimates and standard errors**.\n",
        "\n",
        "*   **Efficiency of Estimates**: When homoscedasticity holds, the Ordinary Least Squares (OLS) method provides the most efficient estimates of the regression coefficients (they have the lowest variance among all linear unbiased estimators).\n",
        "*   **Validity of Standard Errors and Statistical Tests**: The standard errors of the coefficients, which are used to calculate t-statistics and p-values, are based on the assumption of constant error variance. If heteroscedasticity is present, the standard errors will be biased (either too large or too small), leading to incorrect t-statistics and p-values. This can result in incorrect conclusions about the statistical significance of the independent variables.\n",
        "*   **Confidence Intervals**: Similar to hypothesis testing, the confidence intervals for the coefficients will also be incorrect if homoscedasticity is violated.\n",
        "\n",
        "In simple terms, heteroscedasticity means that the model's predictions are less precise for some ranges of the independent variables than for others.\n",
        "\n",
        "**How to Check the Homoscedasticity Assumption?**\n",
        "\n",
        "You can check the homoscedasticity assumption using several methods:\n",
        "\n",
        "1.  **Residual Plots**: The most common way to check for homoscedasticity is to plot the residuals against the predicted values of the dependent variable (or against each independent variable).\n",
        "    *   **Homoscedasticity**: The residuals should be randomly scattered around zero with no discernible pattern and a roughly constant width.\n",
        "    *   **Heteroscedasticity**: Look for patterns in the residual plot, such as a fanning-out shape (where the spread of residuals increases with the predicted values) or a fanning-in shape (where the spread decreases).\n",
        "2.  **Statistical Tests**: Several statistical tests can formally test for heteroscedasticity, such as the Breusch-Pagan test, the White test, or the Goldfeld-Quandt test. These tests provide a p-value to help determine if there is significant evidence of heteroscedasticity.\n",
        "\n",
        "**What to Do if the Homoscedasticity Assumption is Violated (Heteroscedasticity)?**\n",
        "\n",
        "If you find that the homoscedasticity assumption is violated, you have a few options:\n",
        "\n",
        "1.  **Transform the Dependent Variable**: Transforming the dependent variable (e.g., using a logarithmic or square root transformation) can sometimes stabilize the variance of the errors.\n",
        "2.  **Use Weighted Least Squares (WLS)**: Weighted Least Squares is a regression method that can be used when heteroscedasticity is present. It assigns different weights to observations based on their variance, giving less weight to observations with higher variance.\n",
        "3.  **Use Robust Standard Errors**: Some statistical software packages can calculate robust standard errors (also known as heteroscedasticity-consistent standard errors). These standard errors are valid even in the presence of heteroscedasticity and allow for correct inference about the coefficients. This is often a simpler approach than WLS.\n",
        "4.  **Consider Other Models**: If the heteroscedasticity is severe and cannot be addressed by transformations or robust methods, it might indicate that a different type of model is more appropriate for the data.\n",
        "\n",
        "In summary, homoscedasticity is important for obtaining efficient coefficient estimates and valid statistical inferences. Checking residual plots and using statistical tests can help identify heteroscedasticity, and various methods are available to address this issue if it is present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9be11971"
      },
      "source": [
        "### Independence of Errors (No Autocorrelation) Assumption Explained in Detail\n",
        "\n",
        "The independence of errors assumption in linear regression states that the **errors (residuals)** of the model are **independent of each other**. This means that the error for one observation does not influence the error for another observation. **Autocorrelation** (also known as serial correlation) occurs when the errors are not independent, and there is a pattern in the residuals over time or space.\n",
        "\n",
        "**What does Independence of Errors Mean?**\n",
        "\n",
        "Independence of errors means that knowing the value of the error for one data point does not give you any information about the value of the error for another data point. The errors are random and not systematically related to each other.\n",
        "\n",
        "Mathematically, this assumption means that the covariance between any two error terms ($\\epsilon_i$ and $\\epsilon_j$ for $i \\neq j$) is zero: $Cov(\\epsilon_i, \\epsilon_j) = 0$.\n",
        "\n",
        "**Why is Independence of Errors Important?**\n",
        "\n",
        "The independence of errors assumption is particularly important in **time series data** or data where the order of observations matters. Violations of this assumption (autocorrelation) can lead to several problems:\n",
        "\n",
        "*   **Biased Standard Errors**: Autocorrelation causes the standard errors of the regression coefficients to be biased. Positive autocorrelation (where consecutive errors are positively correlated) leads to underestimated standard errors, making the coefficients appear more statistically significant than they actually are (inflated t-statistics and smaller p-values). Negative autocorrelation (where consecutive errors are negatively correlated) leads to overestimated standard errors, making the coefficients appear less significant.\n",
        "*   **Inefficient Estimates**: While the coefficient estimates themselves remain unbiased in the presence of autocorrelation, they are no longer the most efficient estimates (they do not have the minimum variance).\n",
        "*   **Invalid Statistical Inference**: Due to the biased standard errors, hypothesis tests and confidence intervals for the coefficients become unreliable.\n",
        "\n",
        "In essence, if autocorrelation is present, the model is not fully capturing the systematic pattern in the data, and some of that pattern is left in the residuals.\n",
        "\n",
        "**How to Check the Independence of Errors Assumption?**\n",
        "\n",
        "You can check the independence of errors assumption using several methods:\n",
        "\n",
        "1.  **Residual Plots (against time or order)**: If your data has a time component or a natural order, plot the residuals against time or the order of observations. Look for patterns in the residuals, such as a clear trend or a cyclical pattern. A random scatter of residuals around zero suggests independence.\n",
        "2.  **Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) Plots**: These plots are commonly used in time series analysis. The ACF plot shows the correlation between a time series and lagged versions of itself. Significant spikes at different lags indicate autocorrelation.\n",
        "3.  **Durbin-Watson Statistic**: This is a common statistic reported in regression summaries (like the one you generated). It tests for the presence of autocorrelation in the residuals, usually first-order autocorrelation (correlation between consecutive errors).\n",
        "    *   The Durbin-Watson statistic ranges from 0 to 4.\n",
        "    *   A value around 2 suggests no autocorrelation.\n",
        "    *   Values significantly below 2 suggest positive autocorrelation.\n",
        "    *   Values significantly above 2 suggest negative autocorrelation.\n",
        "    *   The interpretation of the Durbin-Watson statistic depends on the sample size and the number of independent variables, and critical values can be found in statistical tables. In your model summary, the Durbin-Watson statistic is 0.948, which is below 2, suggesting positive autocorrelation in the residuals.\n",
        "4.  **Breusch-Godfrey Test**: This is a more general test for autocorrelation that can detect higher-order autocorrelation as well.\n",
        "\n",
        "**What to Do if the Independence of Errors Assumption is Violated (Autocorrelation)?**\n",
        "\n",
        "If you find that the independence of errors assumption is violated, you have a few options:\n",
        "\n",
        "1.  **Include Lagged Variables**: In time series data, you can include lagged values of the dependent variable or the independent variables as predictors in the model to capture the autocorrelation.\n",
        "2.  **Use Time Series Models**: If the autocorrelation is significant and the data is a time series, more specialized time series models (like ARIMA models) might be more appropriate.\n",
        "3.  **Transform the Variables**: Sometimes, transforming the dependent variable or using differencing (calculating the difference between consecutive observations) can help remove autocorrelation.\n",
        "4.  **Use Feasible Generalized Least Squares (FGLS)**: This is a regression method that can be used when autocorrelation is present. It estimates the correlation structure of the errors and uses that information to transform the data and obtain more efficient estimates.\n",
        "5.  **Use Robust Standard Errors (Clustered Standard Errors)**: In some cases, especially when the autocorrelation is clustered within groups of observations, using robust standard errors (specifically clustered standard errors) can provide valid inference even in the presence of autocorrelation.\n",
        "\n",
        "In summary, the independence of errors assumption is vital, particularly for time series data, as its violation can lead to biased standard errors and incorrect statistical inferences. Checking for autocorrelation using residual plots, the Durbin-Watson statistic, or other tests is important, and various methods are available to address this issue if it is present. Based on your model's Durbin-Watson statistic of 0.948, there appears to be some positive autocorrelation in the residuals, which you might want to investigate further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f61c40"
      },
      "source": [
        "### No Multicollinearity Assumption Explained in Detail\n",
        "\n",
        "The no multicollinearity assumption in linear regression states that the **independent variables should not be highly correlated with each other**. **Multicollinearity** occurs when two or more independent variables in a regression model are highly linearly related.\n",
        "\n",
        "**What does No Multicollinearity Mean?**\n",
        "\n",
        "No multicollinearity means that each independent variable provides unique information to the model that is not already explained by the other independent variables. While some degree of correlation between independent variables is expected, high correlation can cause problems.\n",
        "\n",
        "Mathematically, multicollinearity exists when one independent variable can be expressed as a linear combination of other independent variables.\n",
        "\n",
        "**Why is No Multicollinearity Important?**\n",
        "\n",
        "Multicollinearity does **not** affect the overall predictive power of the model (e.g., the R-squared value). However, it does significantly impact the **interpretation and stability of the individual regression coefficients**:\n",
        "\n",
        "*   **Unstable Coefficient Estimates**: In the presence of high multicollinearity, the estimated coefficients for the correlated independent variables can be unstable and highly sensitive to small changes in the data. This means that the sign or magnitude of a coefficient could change dramatically if you add or remove a few data points.\n",
        "*   **Difficulty in Interpreting Individual Effects**: It becomes difficult to determine the individual effect of each correlated independent variable on the dependent variable because their effects are intertwined. The model can tell you that the group of correlated variables is important, but not which specific variable within that group is driving the effect.\n",
        "*   **Inflated Standard Errors**: Multicollinearity inflates the standard errors of the coefficients for the correlated variables. This leads to smaller t-statistics and larger p-values, making it difficult to determine if the individual variables are statistically significant, even if the overall model is significant.\n",
        "*   **Wide Confidence Intervals**: Due to the inflated standard errors, the confidence intervals for the coefficients of the correlated variables become very wide, reflecting the uncertainty in their estimated values.\n",
        "\n",
        "In extreme cases of perfect multicollinearity (where one independent variable is a perfect linear combination of others), the regression model cannot be estimated at all.\n",
        "\n",
        "**How to Check the No Multicollinearity Assumption?**\n",
        "\n",
        "You can check for multicollinearity using several methods:\n",
        "\n",
        "1.  **Correlation Matrix**: Calculate the correlation matrix of your independent variables. Look for high correlation coefficients (typically above 0.7 or 0.8 in absolute value) between pairs of independent variables. This is a good initial check, but it only identifies pairwise correlations, not relationships among three or more variables.\n",
        "2.  **Variance Inflation Factor (VIF)**: The VIF measures how much the variance of the estimated regression coefficient is increased due to multicollinearity. A VIF of 1 means there is no multicollinearity for that variable. A VIF greater than 1 indicates multicollinearity.\n",
        "    *   Commonly used thresholds for VIF are 5 or 10. A VIF above 5 or 10 is often considered an indication of significant multicollinearity.\n",
        "    *   You can calculate the VIF for each independent variable.\n",
        "3.  **Eigenvalues and Condition Index**: Some statistical software provides eigenvalues of the correlation matrix and a condition index. A high condition index (e.g., above 15 or 30) can indicate multicollinearity.\n",
        "\n",
        "**What to Do if the No Multicollinearity Assumption is Violated?**\n",
        "\n",
        "If you find that multicollinearity is present, you have a few options:\n",
        "\n",
        "1.  **Remove One of the Correlated Variables**: If two or more independent variables are highly correlated, you can consider removing one of them from the model. Choose the variable that is less theoretically important or has a weaker relationship with the dependent variable.\n",
        "2.  **Combine Correlated Variables**: You can create a new variable that is a composite of the correlated variables (e.g., by averaging them or using principal component analysis).\n",
        "3.  **Collect More Data**: Multicollinearity can sometimes be reduced by collecting more data, especially if the current data set has limited variability in the independent variables.\n",
        "4.  **Use Ridge Regression or Lasso Regression**: These are penalized regression methods that can handle multicollinearity by shrinking the coefficient estimates.\n",
        "5.  **Standardize Variables**: While standardizing variables (subtracting the mean and dividing by the standard deviation) does not eliminate multicollinearity, it can sometimes help with the interpretation of coefficients when multicollinearity is present.\n",
        "\n",
        "In summary, multicollinearity can make it difficult to interpret the individual effects of independent variables and can lead to unstable and unreliable coefficient estimates and standard errors. Checking for multicollinearity using correlation matrices and VIFs is important, and various methods are available to address this issue if it is present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897441da"
      },
      "source": [
        "### Dummy Variables in Regression Explained in Detail\n",
        "\n",
        "Dummy variables are a way to include **categorical independent variables** (variables that represent groups or categories, like gender, region, or experimental condition) in a linear regression model. Linear regression models require all independent variables to be numerical, so dummy variables are used to convert categorical data into a numerical format that the model can understand.\n",
        "\n",
        "**What is a Dummy Variable?**\n",
        "\n",
        "A dummy variable is a binary variable, meaning it can only take on two values, typically **0** or **1**. These values are used to represent the presence or absence of a specific category.\n",
        "\n",
        "For a categorical variable with *k* categories, you typically create *k-1* dummy variables. The category that does not have a dummy variable assigned to it is called the **reference category** (or base category). The coefficients of the dummy variables are then interpreted in comparison to this reference category.\n",
        "\n",
        "**How are Dummy Variables Created?**\n",
        "\n",
        "Let's say you have a categorical variable called \"City\" with three categories: \"New York\", \"London\", and \"Paris\". To include this in a regression model, you would create *k-1 = 3-1 = 2* dummy variables.\n",
        "\n",
        "You could create two dummy variables:\n",
        "\n",
        "1.  **`City_London`**: This variable would be 1 if the observation is from London, and 0 otherwise (New York or Paris).\n",
        "2.  **`City_Paris`**: This variable would be 1 if the observation is from Paris, and 0 otherwise (New York or London).\n",
        "\n",
        "In this case, \"New York\" would be the reference category.\n",
        "\n",
        "**How are Dummy Variables Used in Regression?**\n",
        "\n",
        "When you include these dummy variables in your regression model, the model equation might look something like this (assuming 'Income' is the dependent variable and 'Years of Experience' is another independent variable):\n",
        "\n",
        "`Income = Î²â + Î²â * Years of Experience + Î²â * City_London + Î²â * City_Paris + Îµ`\n",
        "\n",
        "**Interpretation of Coefficients with Dummy Variables:**\n",
        "\n",
        "*   **Î²â**: This is the intercept. It represents the expected value of the dependent variable (Income) when all independent variables (including the dummy variables) are zero. In this example, it would represent the expected income for someone with zero years of experience who is in the reference category (\"New York\").\n",
        "*   **Î²â**: This is the coefficient for 'Years of Experience'. It represents the expected change in Income for a one-unit increase in Years of Experience, holding City constant.\n",
        "*   **Î²â**: This is the coefficient for `City_London`. It represents the **difference** in expected Income between someone in London and someone in the reference category (\"New York\"), holding Years of Experience constant. If Î²â is positive, it means people in London are expected to earn Î²â more than people in New York, on average, with the same years of experience.\n",
        "*   **Î²â**: This is the coefficient for `City_Paris`. It represents the **difference** in expected Income between someone in Paris and someone in the reference category (\"New York\"), holding Years of Experience constant.\n",
        "\n",
        "**Why Use k-1 Dummy Variables? (The Dummy Variable Trap)**\n",
        "\n",
        "If you were to create a dummy variable for all *k* categories (e.g., `City_New York`, `City_London`, `City_Paris`), you would fall into the **dummy variable trap**. This leads to perfect multicollinearity because the sum of the dummy variables for all categories would always equal 1 (e.g., `City_New York + City_London + City_Paris = 1`). This perfect linear relationship between the independent variables makes it impossible for the regression model to estimate the coefficients uniquely. By excluding one category and using it as the reference, you avoid this issue.\n",
        "\n",
        "**Choosing the Reference Category:**\n",
        "\n",
        "The choice of the reference category is arbitrary and does not affect the overall fit of the model or the predictions. However, it does affect the interpretation of the dummy variable coefficients. It's often helpful to choose a reference category that is meaningful for comparison (e.g., a baseline group or the most common category).\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "Dummy variables are a technique to incorporate categorical independent variables into linear regression models. By converting categories into binary (0/1) variables, you can estimate the effect of each category relative to a chosen reference category. Understanding how to create and interpret dummy variables is essential when working with datasets that contain both numerical and categorical predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "ae4ec02f",
        "outputId": "0f15df6d-b56f-494e-fd8a-84ee458bc527"
      },
      "source": [
        "# Make predictions using the trained model\n",
        "predictions = model.predict(x)\n",
        "\n",
        "# Display the first few predictions\n",
        "print(\"Predicted GPA values:\")\n",
        "display(predictions.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted GPA values:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    3.121933\n",
              "1    3.022717\n",
              "2    3.181457\n",
              "3    3.057441\n",
              "4    3.078939\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.121933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.022717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.181457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.057441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.078939</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}